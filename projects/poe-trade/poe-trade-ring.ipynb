{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hi, Slug!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorry it’s taken me so long to get back to you – work’s gotten crazy. Here’s a summary of what I’m working on:\n",
    "\n",
    "**Neural Networks with Fast AI**\n",
    "First, I’m leveraging Jeremy Howard and Rachel Thomas’s excellent deep learning library, FastAI. The library (built on top of PyTorch) implements a ton of “latest and greatest” techniques for dealing with rapidly creating and training neural networks. \n",
    "\n",
    "*Side note: if you haven’t come across them before, their tagline is “making neural nets uncool again”. From their site:  “Being cool is about being exclusive, and that’s the opposite of what we want. We want to make deep learning as accessible as possible– including to people using uncool languages like C#, uncool operating systems like Windows (which is used by the majority of the world), uncool datasets (way smaller than anything at Google, and in domain areas you’d consider obscure), and with uncool backgrounds (maybe you didn’t go to Stanford).”*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the FastAI library and the tabular module.\n",
    "from fastai import *\n",
    "from fastai.tabular import *\n",
    "\n",
    "# https://github.com/wesm/feather\n",
    "import feather\n",
    "\n",
    "# This will drop once I fix the built-in MSE calc in FastAI\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import ring files\n",
    "# ring_X = pd.read_csv('data/ring_X.csv')\n",
    "# ring_y = pd.read_csv('data/ring_Y.csv')\n",
    "\n",
    "# # Combine X and Y\n",
    "# ring_raw = pd.concat([ring_X, ring_y], axis=1, join='inner')\n",
    "\n",
    "# # Feather ring files\n",
    "# feather.write_dataframe(ring_raw, 'data/ring_raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll send you a separate email with a few questions I have on the columns of the data set, but I'll drop the EDA from this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing and creating the data bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read feather version of data into Pandas\n",
    "ring_raw = feather.read_dataframe('data/ring_raw')\n",
    "\n",
    "# Drop all zero value items. There are less than 200 and they screw with the training too much.\n",
    "ring_raw = ring_raw[ring_raw['y'] != 0]\n",
    "\n",
    "# Set the path to the data source\n",
    "path = './data'\n",
    "\n",
    "# Create a copy of the data frame under the name \"df\". I only do this because I'm lazy and can reuse code easier. \n",
    "df = ring_raw.copy()\n",
    "\n",
    "# Set your dependant variable.\n",
    "dep_var = 'y'\n",
    "\n",
    "# Create two lists: one with low-cardinality columns (less than 50) and high-cardinality. The low ones get treated\n",
    "# as categorical variables (given embedding matricies in the neural network) while the rest are treated as linear. \n",
    "# I'm working on a v2 where I go through each variable and see which would work better where, but for prototyping, \n",
    "# this is enough.\n",
    "cat_vars = []\n",
    "cont_vars = []\n",
    "\n",
    "for i in list(ring_raw.columns):\n",
    "    if df[i].nunique() < 50: cat_vars.append(i)\n",
    "    else: \n",
    "        if i != dep_var: cont_vars.append(i)\n",
    "\n",
    "# Set which pre-processing steps FastAI will handle.            \n",
    "procs = [Normalize, Categorify, FillMissing]\n",
    "\n",
    "# Creating 20% validation set. Here too I'm being lazy - future models should have a more thought out validation set\n",
    "# that accuarately reflects the overall distribution.\n",
    "valid_idx = range(len(ring_raw)-64900, len(ring_raw))\n",
    "\n",
    "# Create the Data Bunch - automates the creation of the data loaders, does pre-processing, and comes with a few helper functions as well...\n",
    "data = (TabularList.from_df(df, path=path, cat_names=cat_vars, cont_names=cont_vars, procs=procs)\n",
    "                .split_by_idx(valid_idx)\n",
    "                .label_from_df(cols=dep_var, log=True) # Given the massive positive skew of the prices, we'll use the log of the target\n",
    "                .databunch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <col width='10%'>  <tr>\n",
       "    <th>Curse Enemies with Level # Despair on Hit</th>\n",
       "    <th>+# Life gained for each Enemy hit by your Attacks</th>\n",
       "    <th>#% of Energy Shield Regenerated per second</th>\n",
       "    <th>+#% to all Elemental Resistances</th>\n",
       "    <th>Curse Enemies with Level # Frostbite on Hit</th>\n",
       "    <th>(pseudo) (total) Adds # Physical Damage to Attacks</th>\n",
       "    <th>Grants Level # Despair Skill</th>\n",
       "    <th>+# to Armour</th>\n",
       "    <th>#% faster start of Energy Shield Recharge</th>\n",
       "    <th>#% increased Strength</th>\n",
       "    <th>#% increased Area of Effect of Curse Skills</th>\n",
       "    <th>Gain #% of Physical Damage as Extra Fire Damage</th>\n",
       "    <th>#% increased Intelligence</th>\n",
       "    <th>+# Mana gained on Kill</th>\n",
       "    <th>Grants Level # Hatred Skill</th>\n",
       "    <th>Cannot be Ignited</th>\n",
       "    <th>#% increased Global Physical Damage</th>\n",
       "    <th>#% of Cold Damage Leeched as Life</th>\n",
       "    <th>#% increased Light Radius</th>\n",
       "    <th>Minions have #% increased Movement Speed</th>\n",
       "    <th>Wrath has #% increased Aura Effect</th>\n",
       "    <th>Grants Level # Aspect of the Cat Skill</th>\n",
       "    <th>Curse Enemies with Level # Vulnerability on Hit</th>\n",
       "    <th>#% increased maximum Energy Shield</th>\n",
       "    <th>Grants Level # Wrath Skill</th>\n",
       "    <th>Grants Level # Aspect of the Avian Skill</th>\n",
       "    <th>Hatred has #% increased Aura Effect</th>\n",
       "    <th>#% increased Damage</th>\n",
       "    <th>#% of Physical Attack Damage Leeched as Life</th>\n",
       "    <th>#% of Lightning Damage Leeched as Life</th>\n",
       "    <th>Anger has #% increased Aura Effect</th>\n",
       "    <th>Grace has #% increased Aura Effect</th>\n",
       "    <th>+# to Minimum Power Charges</th>\n",
       "    <th>Properties are doubled while in a Breach</th>\n",
       "    <th>edps</th>\n",
       "    <th>Item sells for much more to vendors</th>\n",
       "    <th>(pseudo) (total) +#% to Global Critical Strike Multiplier</th>\n",
       "    <th>Curse Enemies with Level # Flammability on Hit</th>\n",
       "    <th>+# Life gained on Kill</th>\n",
       "    <th>Determination has #% increased Aura Effect</th>\n",
       "    <th>-# to Total Mana Cost of Skills</th>\n",
       "    <th>(pseudo) (total) #% increased Chaos Spell Damage</th>\n",
       "    <th>#% of Physical Attack Damage Leeched as Mana</th>\n",
       "    <th>#% of Physical Damage Leeched as Life</th>\n",
       "    <th>Adds # to # Physical Damage to Attacks</th>\n",
       "    <th>Cannot roll Caster Modifiers</th>\n",
       "    <th># to # Cold Damage per Frenzy Charge</th>\n",
       "    <th>Grants Level # Aspect of the Spider Skill</th>\n",
       "    <th>#% of Damage taken gained as Mana over 4 seconds when Hit</th>\n",
       "    <th>#% increased Global Accuracy Rating</th>\n",
       "    <th>Discipline has #% increased Aura Effect</th>\n",
       "    <th>(pseudo) (total) #% increased Cast Speed</th>\n",
       "    <th>Curse Enemies with Level # Conductivity on Hit</th>\n",
       "    <th>Grants Level # Anger Skill</th>\n",
       "    <th>+# Mana gained for each Enemy hit by your Attacks</th>\n",
       "    <th>#% increased Dexterity</th>\n",
       "    <th>+#% chance to Evade Attacks</th>\n",
       "    <th>Grants Level # Frostbite Skill</th>\n",
       "    <th>(pseudo) (total) #% increased Attack Speed</th>\n",
       "    <th>+# to Level of Socketed Gems</th>\n",
       "    <th># Mana Regenerated per second</th>\n",
       "    <th>#% increased Effect of non-Damaging Ailments on Enemies</th>\n",
       "    <th>+# to Minimum Endurance Charges</th>\n",
       "    <th>#% chance when Hit for double Armour effect</th>\n",
       "    <th>#% increased Damage with Ailments</th>\n",
       "    <th>Cannot be Shocked or Ignited while moving</th>\n",
       "    <th>Cannot be Poisoned</th>\n",
       "    <th>#% increased Curse Duration</th>\n",
       "    <th>(pseudo) # Elemental Resistances</th>\n",
       "    <th>links</th>\n",
       "    <th>(pseudo) (total) #% increased Global Critical Strike Chance</th>\n",
       "    <th>#% increased maximum Mana</th>\n",
       "    <th>#% reduced Mana Cost of Skills</th>\n",
       "    <th>Grants Level # Aspect of the Crab Skill</th>\n",
       "    <th>#% of Chaos Damage Leeched as Life</th>\n",
       "    <th>#% of Fire Damage Leeched as Life</th>\n",
       "    <th>Grants Level # Flammability Skill</th>\n",
       "    <th>(pseudo) (total) #% increased Critical Strike Chance for Spells</th>\n",
       "    <th>#% increased Energy Shield from Body Armour</th>\n",
       "    <th>Suffixes Cannot Be Changed</th>\n",
       "    <th>#% increased Quantity of Items found</th>\n",
       "    <th>dps</th>\n",
       "    <th>Can have multiple Crafted Modifiers</th>\n",
       "    <th>Minions deal #% increased Damage</th>\n",
       "    <th>Grants Level # Conductivity Skill</th>\n",
       "    <th>Shock nearby Enemies for # Seconds when you Focus</th>\n",
       "    <th>+#% to Critical Strike Multiplier if you've Shattered an Enemy Recently</th>\n",
       "    <th>#% increased maximum Life</th>\n",
       "    <th>#% of Life Regenerated per second</th>\n",
       "    <th>#% increased Life Recovery from Flasks</th>\n",
       "    <th>+# to Minimum Frenzy Charges</th>\n",
       "    <th>#% increased Damage with Poison</th>\n",
       "    <th>(pseudo) (total) #% increased Lightning Damage with Attack Skills</th>\n",
       "    <th>(pseudo) (total) #% increased Fire Damage with Attack Skills</th>\n",
       "    <th>Adds # to # Fire Damage to Attacks</th>\n",
       "    <th>Adds # to # Cold Damage to Attacks</th>\n",
       "    <th>+# to Accuracy Rating</th>\n",
       "    <th>buyout_hits</th>\n",
       "    <th>corrupted</th>\n",
       "    <th>(pseudo) (total) #% increased Elemental Damage with Attack Skills</th>\n",
       "    <th>(pseudo) (total) +# to maximum Life</th>\n",
       "    <th>(pseudo) (total) +#% to Fire Resistance</th>\n",
       "    <th>(pseudo) (total) +#% to Lightning Resistance</th>\n",
       "    <th>(pseudo) (total) #% increased Fire Spell Damage</th>\n",
       "    <th>ilvl</th>\n",
       "    <th>(pseudo) (total) +#% to Chaos Resistance</th>\n",
       "    <th>(pseudo) (total) +# to maximum Mana</th>\n",
       "    <th>(pseudo) +#% total Elemental Resistance</th>\n",
       "    <th>(pseudo) (total) Adds # Fire Damage to Attacks</th>\n",
       "    <th>(pseudo) (total) +# to Dexterity</th>\n",
       "    <th>(pseudo) (total) Adds # Elemental Damage to Attacks</th>\n",
       "    <th>(pseudo) (total) +# to Intelligence</th>\n",
       "    <th>Adds # to # Lightning Damage to Attacks</th>\n",
       "    <th>(pseudo) (total) #% increased Burning Damage</th>\n",
       "    <th>+# to maximum Energy Shield</th>\n",
       "    <th>(pseudo) (total) +#% to Cold Resistance</th>\n",
       "    <th>#% increased Mana Regeneration Rate</th>\n",
       "    <th>+# to Evasion Rating</th>\n",
       "    <th>(pseudo) (total) #% increased Rarity of Items found</th>\n",
       "    <th>(pseudo) (total) #% increased Cold Damage with Attack Skills</th>\n",
       "    <th>(pseudo) (total) #% increased Lightning Spell Damage</th>\n",
       "    <th># Life Regenerated per second</th>\n",
       "    <th>(pseudo) (total) +# to Strength</th>\n",
       "    <th>(pseudo) (total) #% increased Cold Spell Damage</th>\n",
       "    <th>(pseudo) (total) Adds # Elemental Damage to Spells</th>\n",
       "    <th>target</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>15.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>3.5224</th>\n",
       "    <th>1.9132</th>\n",
       "    <th>-0.3331</th>\n",
       "    <th>-0.3310</th>\n",
       "    <th>-0.3701</th>\n",
       "    <th>0.2442</th>\n",
       "    <th>-0.2540</th>\n",
       "    <th>1.5010</th>\n",
       "    <th>-0.9732</th>\n",
       "    <th>-0.8843</th>\n",
       "    <th>-0.8853</th>\n",
       "    <th>4.0829</th>\n",
       "    <th>0.5648</th>\n",
       "    <th>-0.2906</th>\n",
       "    <th>-0.7106</th>\n",
       "    <th>-1.3997</th>\n",
       "    <th>-0.3427</th>\n",
       "    <th>-0.4291</th>\n",
       "    <th>-0.6148</th>\n",
       "    <th>-0.4450</th>\n",
       "    <th>-0.3489</th>\n",
       "    <th>-0.1668</th>\n",
       "    <th>1.5913</th>\n",
       "    <th>-0.8871</th>\n",
       "    <th>-0.3221</th>\n",
       "    <th>-0.3230</th>\n",
       "    <th>-0.4279</th>\n",
       "    <th>1.3489</th>\n",
       "    <th>7.1458</th>\n",
       "    <th>0.3329</th>\n",
       "    <th>-0.4440</th>\n",
       "    <th>3.0652</th>\n",
       "    <th>-0.3176</th>\n",
       "    <th>4.248495101928711</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>3.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.3</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>3.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>21.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>-0.4233</th>\n",
       "    <th>-0.4261</th>\n",
       "    <th>-0.3331</th>\n",
       "    <th>-0.3310</th>\n",
       "    <th>-0.3701</th>\n",
       "    <th>0.2442</th>\n",
       "    <th>-0.2540</th>\n",
       "    <th>-0.3780</th>\n",
       "    <th>-0.9732</th>\n",
       "    <th>-0.7141</th>\n",
       "    <th>-0.7152</th>\n",
       "    <th>-0.2841</th>\n",
       "    <th>-0.8470</th>\n",
       "    <th>-0.2906</th>\n",
       "    <th>-0.3294</th>\n",
       "    <th>-1.1310</th>\n",
       "    <th>-0.3427</th>\n",
       "    <th>-0.4291</th>\n",
       "    <th>-0.6148</th>\n",
       "    <th>0.9381</th>\n",
       "    <th>-0.3489</th>\n",
       "    <th>-0.1668</th>\n",
       "    <th>-0.5674</th>\n",
       "    <th>-0.7173</th>\n",
       "    <th>-0.3221</th>\n",
       "    <th>-0.3230</th>\n",
       "    <th>-0.4279</th>\n",
       "    <th>-0.4213</th>\n",
       "    <th>-0.2786</th>\n",
       "    <th>-0.2715</th>\n",
       "    <th>-0.4440</th>\n",
       "    <th>-0.2733</th>\n",
       "    <th>-0.3176</th>\n",
       "    <th>1.6094379425048828</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0.0</th>\n",
       "    <th>2.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>6.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>3.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>-0.4233</th>\n",
       "    <th>-0.4261</th>\n",
       "    <th>1.4565</th>\n",
       "    <th>-0.3310</th>\n",
       "    <th>-0.3701</th>\n",
       "    <th>0.2442</th>\n",
       "    <th>-0.2540</th>\n",
       "    <th>-0.3780</th>\n",
       "    <th>0.0605</th>\n",
       "    <th>-0.5439</th>\n",
       "    <th>1.1548</th>\n",
       "    <th>-0.2841</th>\n",
       "    <th>-0.3428</th>\n",
       "    <th>-0.2906</th>\n",
       "    <th>0.3061</th>\n",
       "    <th>1.3765</th>\n",
       "    <th>1.4210</th>\n",
       "    <th>-0.4291</th>\n",
       "    <th>0.3888</th>\n",
       "    <th>-0.4450</th>\n",
       "    <th>-0.3489</th>\n",
       "    <th>-0.1668</th>\n",
       "    <th>-0.5674</th>\n",
       "    <th>1.9984</th>\n",
       "    <th>-0.3221</th>\n",
       "    <th>-0.3230</th>\n",
       "    <th>-0.4279</th>\n",
       "    <th>-0.4213</th>\n",
       "    <th>-0.2786</th>\n",
       "    <th>-0.2715</th>\n",
       "    <th>-0.4440</th>\n",
       "    <th>-0.2733</th>\n",
       "    <th>-0.3176</th>\n",
       "    <th>0.0</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>12.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>2.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>-0.4233</th>\n",
       "    <th>-0.4261</th>\n",
       "    <th>-0.3331</th>\n",
       "    <th>-0.3310</th>\n",
       "    <th>-0.3701</th>\n",
       "    <th>0.2442</th>\n",
       "    <th>-0.2540</th>\n",
       "    <th>-0.3780</th>\n",
       "    <th>-0.3310</th>\n",
       "    <th>-0.1468</th>\n",
       "    <th>-0.8853</th>\n",
       "    <th>-0.2841</th>\n",
       "    <th>0.7665</th>\n",
       "    <th>-0.2906</th>\n",
       "    <th>1.5135</th>\n",
       "    <th>-0.6236</th>\n",
       "    <th>-0.3427</th>\n",
       "    <th>-0.4291</th>\n",
       "    <th>-0.6148</th>\n",
       "    <th>1.1686</th>\n",
       "    <th>-0.3489</th>\n",
       "    <th>-0.1668</th>\n",
       "    <th>-0.5674</th>\n",
       "    <th>-0.1516</th>\n",
       "    <th>-0.3221</th>\n",
       "    <th>5.3733</th>\n",
       "    <th>-0.4279</th>\n",
       "    <th>-0.4213</th>\n",
       "    <th>-0.2786</th>\n",
       "    <th>-0.2715</th>\n",
       "    <th>2.7300</th>\n",
       "    <th>-0.2733</th>\n",
       "    <th>-0.3176</th>\n",
       "    <th>0.6931471824645996</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>14.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>3.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>0.0</th>\n",
       "    <th>-0.4233</th>\n",
       "    <th>-0.4261</th>\n",
       "    <th>-0.3331</th>\n",
       "    <th>-0.3310</th>\n",
       "    <th>-0.3701</th>\n",
       "    <th>0.2442</th>\n",
       "    <th>-0.2540</th>\n",
       "    <th>-0.3780</th>\n",
       "    <th>0.1858</th>\n",
       "    <th>3.0865</th>\n",
       "    <th>-0.0919</th>\n",
       "    <th>-0.2841</th>\n",
       "    <th>0.3631</th>\n",
       "    <th>-0.2906</th>\n",
       "    <th>-0.7106</th>\n",
       "    <th>1.5257</th>\n",
       "    <th>-0.3427</th>\n",
       "    <th>-0.4291</th>\n",
       "    <th>-0.6148</th>\n",
       "    <th>-0.4450</th>\n",
       "    <th>-0.3489</th>\n",
       "    <th>-0.1668</th>\n",
       "    <th>-0.0799</th>\n",
       "    <th>-0.0950</th>\n",
       "    <th>-0.3221</th>\n",
       "    <th>-0.3230</th>\n",
       "    <th>-0.4279</th>\n",
       "    <th>-0.4213</th>\n",
       "    <th>-0.2786</th>\n",
       "    <th>-0.2715</th>\n",
       "    <th>-0.4440</th>\n",
       "    <th>-0.2733</th>\n",
       "    <th>-0.3176</th>\n",
       "    <th>0.6931471824645996</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ... suh as this one.\n",
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RMSE function to be printed out after each epoch below.\n",
    "def rmse(pred, targ):\n",
    "    \"RMSE between `pred` and `targ`.\"\n",
    "    return torch.sqrt(((targ - pred)**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a four-hidden-layer NN with 400, 200, 100, 50 perceptrons in each respective layer. I'm still playing with architecture, so \n",
    "# this will almost certianly change.\n",
    "learn = tabular_learner(data, layers=[400,200,100,50], metrics=rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEKCAYAAADn+anLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAIABJREFUeJzt3Xd8VFX+//HXJ5NeSAJJID1A6B1CB6UogvrD7oqLYmUtu7rruq5t/Squfd3VXdeCfa2oKCgqKCKIUkOVDqEnQBIgISE9Ob8/ZoAQkpAyNzfl83w85sGde+/MvDNM5pNzz73niDEGpZRSqjIedgdQSinVeGmRUEopVSUtEkoppaqkRUIppVSVtEgopZSqkhYJpZRSVfK08slFZDeQA5QCJcaYpArbRwGzgV2uVZ8bY6ZZmUkppVTNWVokXEYbYzKr2b7YGHNxA+RQSilVS3q4SSmlVJWsbkkY4DsRMcBrxpjplewzVETWAWnAvcaYjdU9YVhYmElISHB/UqWUasZWrVqVaYwJr+3jrC4Sw40xaSISAXwvIluMMT+V274aiDfG5IrIhcAsoFPFJxGRqcBUgLi4OJKTky2OrZRSzYuI7KnL4yw93GSMSXP9mw58AQyqsP2YMSbXtfwN4CUiYZU8z3RjTJIxJik8vNaFUCmlVB1ZViREJEBEgk4sA+OADRX2aSci4loe5Mpz2KpMSimlasfKw01tgS9cNcAT+NAYM1dEbgMwxrwKXAncLiIlQD5wjdFhaZVSqtGwrEgYY3YCfSpZ/2q55ZeAl6zKoJRSqn70FFillFJV0iKhlFKqSloklFJKVUmLRAXGGD5ZuY+C4tKT68rKtC9dKdUyaZGoYNnOI9w3cz2PfbUJgOk/pdDnse9Iy8q3OZlSSjU8LRIVeIjz38XbMwD4ev0BcgpL+NusDdU8SrnT5gPH2JV53O4YSim0SJyhoKQMgP1H87n5nZWs258NwIKt6ew/mmdntBahrMww4cXFjP7HQvSSGaXsp0WigvJ9ET9sSQdgQs92CPDxin02pWr+th7MYfwLP9HhwW9Orpu/OZ3svGKKS8tsTKZUy6ZFooLyReKEIR3aMKZrBB+v3FenL6zCktI6/1VsjOHGt1fw3tLdlW7fkZ7LviNVt3COFRSTnV9cp9d2tx3puQx8Yj6r9hw9ue7n7Znc8cEqLnjhJzJyCvH2dH4ko4J9uf39VSQ98T03vbOSbYdytF9IKRs0xKRDTUp+0ZlFIsjXk4l9o5m/OZ2tB3PoGR1c4+c7XljChBcXExXiyyMX96B7VKta5UnJyOXHrRn8uDWD7zen89aUJBweQlFpGU99s4V3luwmMtiXRX8ZffILtrzJbyxn/f5sNk27AAB/76r/y48XluDpEHw8HbXKWFMfLt9LRk4hf525nvn3nMu2Qznc9M5KDIYQfy8+mjqE2FB/Dh8vpLTM8OQ3m/llx2EWb89k3L9+okNYAPPvOZfHv95EdIgfca39OXisgOuHJliSVymlReIM+RVaEtcOjuOCHu04eKwAgC21KBJLUw4z6fVlAOw9ksfEl35m+YNjaRPoU+3jsvKKmP7TTgzg5epJ7xAWwE/bMkh86NvT9m0fFsCuzONc/dpSXpncn0dmb6RTRCCRIX5MGhjLelefSo//m4cAqx4+n9AA7zNec+2+LK5+bSkTerbjxWv61ejnq42yMsO3Gw4AzhbFjvRcpn21iQAfB/PvOZfWAd64xvkixtsfgNeuc852u2RHJs/M28q6fVn8b+lu3v5l92nP/cjsjbx70yDO7awjBCvlbnq4qYKC4tMPJz15WS8CfDxJaBOAr5cHmw8cO+tzlJUZCktKmev6Uvzbxd15dXJ/SsoMy3YeOevjn523lZcXpvDKwhT+vWAHHcOdf0F3igg8bb+rk2JY8OdzuWFYAmv3ZXHN9GV8v+kQLy9M4W+zNpw8jRfAGCgzMPW9ZA65Ch7AnsPH2ZV5nP/+uIOikjJmr00j1YLDOst3HeFAdgEPXtgVgPP+uYifd2Ryx6hE2gT6nCwQlRmWGMaMqUPoEBbAo+V+phB/Lzq3db4nU95awb2frnN7bqVaOm1JVHCiJbHk/jGnrXd4CF3aBtWoSPz968289csu2ocFcE7ncG4e0Z7i0jICvB0s3ZnJRb0jq3386j1HGdkpjGsHxfHKohSevKwXHh7Cp7cNRUQ4ll9MTKjfyS/WRyf2IDO3kDnrnUVp0qBYftqWyXvLnHOMXJ0UwyfJ+xncvjXr92fz50/W8caUJHw8PbjilaVk5hYCMCIxjCUpmby3dA/3T+hauzfuLOasT8Pf28HkIfEs2pbBr/uzuWJADJOHxNfo8b5eDr6+ayTPf7eVfUfz+P3oTnSMCMDf25M9h49z4zsrmbl6P38Yk0h8mwC3ZleqJdMiUUFBcSm+Xh5EhfidsS0poTXvLd1DZm4hYRUOGWXmFjJ7bRoZOYW89csuAHZlHueagbEAeDk8GNoxjLkbDvHQhd3x8678uH9+USnb03M5v3tbJvSKZEKvUwUlxN95mCjYz+uMxz1+SU/CAn3oGR3MlQNi+Of32/j3D9uJCfXj8Ut7MjChNZf1i+aT5P08+MWvXPLSL9w/oevJAgFwab9oAn08eXVRCtGhflxXwy/wmli9N4uBCa3x9/bkzSkD8fQQPB21a8j6eTt4+OLuZ6yPbxPAR7cOYdjTC/hs1X7+PK6Lu2Ir1eLp4aYKCopL8fOq/At80qA4ikrL+DR5/8l1s9em8tfP1vP0t1t4fM4mXl2Uctpjzu/e9uTy1HM6kJlbyAfLT80i+MHyPfz5k3XkF5WSU1DMzNX7KS0z9KpF5zhAaIA3j07swZUDYgAY3cV5fP6m4e3x8XRwVVIsng4Prh0cx5tTkkjJyGXqe8kE+Xry4a2D6Rsbwqgu4fxlfBd6RQfzt1kbeK3Cz1JXRSVl7EjPoYer097Xy1HrAnE2bVv5khQfyvebDrn1eZVq6bQlUUF+USm+VRSJxIhAkuJD+XJdGreP6siG1Gzu/njtye1tW/lw/4SuRAT58ts3lgPQIfxUP8Kg9q0ZntiGVxel8NvB8Xg6hIe+cF7JPXP1qcLTKzqYYYlnzOJaK/3iQll47yji2/ifsW1st7Y8OrEHD8/awGMTuzGsYxiz7nS+XligD7PuHM7dH6/hqW+30DM6mOH1zLLtUA7FpabWZ3bV1vnd2/L3rzez70gesa3P/LmVUrWnLYkK8qtpSQBc2CuSzQeO8e2vB7j4Pz8Dzg7U0V3C+ey2YVzWL4bhiWHMvH0oc/4w4ozH3zkqkczcIoY+/QNjn18EQOtyZxs9e0VvZt05nECf+tfvhLCAKjuEJw+JZ8VDY7l2cNwZ2xwewvNX9yEiyIeXF+6o8esZY9iQmk1phQERl+9ydtb3iKpd66i2TrTavt90iKKSMu76aA1T/5esV24rVQ/akqigoLisypYEwCV9o3h98U5u/2A1ADePaM+DF3bD4XH6l/GA+NaVPn5whzYAZOUVk5VXTIewAKZfP4Apb63kP9f2o39cqJt+krOLCPKtcpuPp4NrB8fxwvztZOUVnewPqc6bP+/i719vxuEh/H50In86vzN5RSW8snAHgxJak1BJq8ad4tsEkBgRyLQ5m5g259RZUF//eoCLe0dZ+tpKNVctpiWxMyOX//64o9KL5co70XFdlTaBPnx065CT9x++6MwCUR2Hh5zsb7h+aDzv3DiIxIggfrl/TIMWiJoY3N5Z0NbszTpjW1pWPofLdXrvO5LHs3O3AlBaZnjxh+3c+eFquj8yj8zcIv50fudqT3N1lynDEvBxXVR4zcBYYkL9mLUm1fLXVaq5ajEtiR3puTw3bytDO7ap9ss4v7i0yjOPTkgIC+DtGwbi4+lRpy++N29IYndmHoPaV97aaCz6xDqL2Y3vrOS9mwcxspOzM3z7oZyTh9pm3j6MntHBPDN3Cw4PYdl9Y/H29GDkMwv42nVKLsCQDg3zs143JJ7Jg+NYuvMwA+JDeXzOJj5fnUpRSVmlV6QrparXYorEiU7TjWnHqi8SRaWE+p95imlFo7tG1DlLRJBvtYd6Ggt/b08GtW/Nil1H+NOMtfx5XBf8vBzMWLmPQtdoufd8spaYUH8WbEnnrrGdaBfs/Llm/34En67aR1iAD90iWzVIK+IEEWFYR2dn+7mdI3h/2V6W7TzMOXpFtlK11mKKRHSIH8F+XmxKy652v4KSqs9uaoneumEg2w/lcNnLS3jg819Prr/n/M7sOZzHzNX7OXSskMv7R/O7czqc3J4YEcgDE7rZEfk0IzuFEeTryeer92uRUKoOWkyREBF6RLViY9oxCktK8fF0kJ5TwD/mbeXBC7uxZl8WN72zEk8PaXR9A3YK9PGkX1wo7988mI1p2STvOcr3mw5x5YAYArw9ubRfFH1jQwjyPXvryw6+Xg4u6RvFp8n7mVZQTKtGmlOpxsrSIiEiu4EcoBQoMcYkVdguwIvAhUAecIMxZrVVefrEhvDKwhS6PDyXt28cyIwV+5i78SCd2wYxe20axkBxqanR4aaWZkSnMEZ0CuOWMkNaVv7JK9JP9FM0ZlcNiOX9ZXuZs+5Apaf8KqWq1hA9eaONMX0rFgiXCUAn120q8IqVQcqPErp4WyYLXJMKLdqWcdqcCyfO6lFncnhIk7tQrXdMMF3bBfHC/G06u6BStWT36R6XAP8zTsuAEBGpfvS7ehgQf+ow0rKdhykqLWNAfCiLt2eyt9zEPYMb6Ewc1TBEhBev6UdOQQl3frCa95ftqXaiJqXUKVYXCQN8JyKrRGRqJdujgfJzgu53rTuNiEwVkWQRSc7IyKhzGC+HB7PuHA7ApgPH8Pd28Np1Awjy8SQiyIdpl/Rg6jkdGu3xdVV3XdoFMWlQHOv2Z/PwrA3c//l6uyMp1SRY3XE93BiTJiIRwPcissUY81O57ZWdF3nGGArGmOnAdICkpKR6jbHQNzaEThGBbE/PpW9sCGGBPvz4l1H4eHpocWjm7h7biagQX7YfymVG8j5mr03lkr5n/E2ilCrH0paEMSbN9W868AUwqMIu+4HYcvdjgDQrMwG0CXQOMdE+zDnvQFigjxaIFiDY34tbRnbgz+M6ExXsy90fr+WWd1eyIz3X7mhKNVqWFQkRCRCRoBPLwDhgQ4XdvgSuF6chQLYx5gAW8/Rw/tgnioRqWSJa+bLg3lH0iGrF/M3pnPfPRTw3b4sOBKhUJaxsSbQFfhaRdcAK4GtjzFwRuU1EbnPt8w2wE9gBvA7cYWGek3IKnGcyJegMZi2Wr5eDT28byn+v7Q/Af39Mofej3/HDZp2PQqnyLOuTMMbsBPpUsv7VcssGuNOqDFXJLSwBIDr0zNnnVMvh7+3JRb0juaDHBK54ZQnr9mcz9b1VvDElidFd6j7silLNid2nwNri+av7Mr5HOxIjAs++s2r2PB0efHHHcDY8dgFd2gbxu/dW8cDn6zl6vMjuaErZrkUWib6xIbx63QC83DyFpmq6PDyEQB9P3r9lMCMSw/hoxT4+Sd539gcq1czpt6RS5bQO8OatGwbSJyaYOeWGOi8rM8xak8pri1LOmHlPqeasxQzwp1RtXN4/hv/7ciPvLtkNwNPfbiG/2DlhlcNDuGVkh2oerVTzoUVCqUpcNySeeRsP8uIP23F4CPnFpTx3ZW8+X53K37/eTEZuYaMYCl0pq+nhJqUq4eEh/GZgLEeOF5GRU8jr1ydxVVIsb96QxFUDYnht0U6Wphy2O6ZSltMioVQVxnSNIMTfiwt6tGWsayZCf29PHr+0J9Ehfjz21Ua9AE81e1oklKpCkK8Xyx4Yy2vXJeHhcWqYMV8vB3ef14ktB3NYvfeojQmVsp4WCaWqUdVUthf2isTXy4O3f9lNmZ7tpJoxLRJK1UGgjyc3j2jPnPUHeH/5HrvjKGUZLRJK1dG947rQOyaYGSv1ojvVfGmRUKqORITL+kWzMe0YryxMsTuOUpbQIqFUPUwaFMeEnu14Zu4W1u/PsjuOUm6nRUKpevD1cvDslb0JC/Rmylsr2Howx+5ISrmVFgml6inI14uPpw6lpNTw3x932B1HKbfSIqGUGyRGBHL1wFi++fUAB7ML7I6jlNtokVDKTaYMTaDUGO7/fD3zNh7kzg9Xs3L3EQCy84opKimzOaFStacD/CnlJnFt/Lm8XwwzV+9n4dYMAOZvOsTr1ydx6/+SGdkpnDemJNmcUqna0ZaEUm703JW9+U1SLAC/O7cD0aF+XP/WCgpLypi/+RAb07IB+GjFXsb9axGz1qSyKe0Yy3fqYIGqcZKmNkBZUlKSSU5OtjuGUlXKKyrhg2V7mTwknqN5RTz5zWYMsHBLOuN6tOOv47sy7l+LOFZQctrjVj18Hm0CfewJrZo9EVlljKl1U1aLhFIN5NEvN/KOaxIjgOev6sOri1IwwI70XO4ak8g947rYlk81b3UtEtonoVQDuXN04ski8eGtgxnWMYwrBsQAcMPbK5i+eCfB/t5EBfsyoVekjUmVOsXyIiEiDiAZSDXGXFxh2w3Ac0Cqa9VLxpg3rM6klB3Cg3yYfedwSsrKGBDf+rRtv0mKZeHWDB6fswmAL+4YxvzNh+gfF8rYbm1P7ne8sAQPEfy8Kx+dVil3a4iWxN3AZqBVFdtnGGN+3wA5lLJdn9iQStdf0KMdL13bD39vB/d99iuXvbwEgA7hAYzpGoGIkFNQzPgXFhPg4+Dhi7ozslMYIlLp8ynlLpYWCRGJAS4CngDusfK1lGrKPDyEi3tHAfD69d7847utxLX256MV+3hh/na2Hsxh7saDJ/e//q0VTL9uAON6tLMrsmohrD4F9gXgPqC6q4iuEJH1IvKZiMRanEepRq9fXCgf3DKEv47vigi8+MP2kwXid+d04CJXf8UbP++yM6ZqISxrSYjIxUC6MWaViIyqYrevgI+MMYUichvwLjCmkueaCkwFiIuLsyixUo1LiL83r04ewMsLUxiZGMb/6xNFl3ZBAPRYuINn525lz+HjxLcJsDmpas4sOwVWRJ4CrgNKAF+cfRKfG2MmV7G/AzhijAmu7nn1FFilIDUrn+FPLyAq2JcZvxtKbGt/uyOpRq6up8BadrjJGPOAMSbGGJMAXAMsqFggRKT8eX4TcXZwK6XOIjrEjztGdSQtu4Bn5m6xO45qxhr8OgkRmQYkG2O+BO4SkYk4WxtHgBsaOo9STdV947sC8MqiFO47nEdcG21NKPfTK66VasIOZOcz9KkFAHx06xCGdmxjcyLVWDW6w01KKetFBvtxcW/nUdsb39GZ8ZT7aZFQqon7z6R+rHhwLL5eDh6ZvYGmdnRANW5aJJRq4kSEiFa+/HFsJ5bvOsKafVmUlWmhUO6hRUKpZuKqpFiCfDy5/OUl9Pi/ebz9i15sp+pPi4RSzUSAjyf3jOsMQH5xKU99s4Wjx4tsTqWaOi0SSjUjNwxL4NXJ/fno1iEUlZYxI3mf3ZFUE6fzSSjVjIgI43s6z3Ya2SmMZ+duoVd0MMMTw2xOppoqbUko1Uy9dt0A4lr787fZGyjVjmxVR1oklGqm/L09+eN5ndmZcZzVe4/aHUc1UVoklGrGzuveFm9PD75ef8DuKKqJ0iKhVDMW6OPJhJ7teG/ZHpbtPGx3HNUEaZFQqpl74rJehPp78cHyvXZHUU2QFgmlmrlAH09GdYlg0dZ0SkqrmyRSqTNpkVCqBRjbNYJjBSWs2H3E7iiqidEioVQLcG6XcPy8HNqBrWpNi4RSLYC/tydju0Xw7YaDeshJ1YoWCaVaiIt7R3HkeBFLUvQsJ1VzWiSUaiFGdQkn0MeTN3/epVdgqxrTIqFUC+Hr5eDecZ1ZtC2DWWtS7Y6jmggtEkq1IFOGJdC2lQ8LtqbbHUU1EVoklGpBRISRncL5eXumHnJSNaJFQqkWZnSXCLLzi1muw3SoGrC8SIiIQ0TWiMicSrb5iMgMEdkhIstFJMHqPEq1dGO7RRDk48lnq/fbHUU1AQ3Rkrgb2FzFtpuBo8aYROBfwDMNkEepFs3Xy8Fl/aOZvTaNjWnZdsdRjZylRUJEYoCLgDeq2OUS4F3X8mfAWBERKzMppeCe8zvTyteT//yww+4oqpGzuiXxAnAfUNUlntHAPgBjTAmQDbSxOJNSLV6IvzeX94/hhy2HOHK8yO44qhGzrEiIyMVAujFmVXW7VbLujFMuRGSqiCSLSHJGRobbMirVkl05IIbiUsPstXrNhKqalS2J4cBEEdkNfAyMEZH3K+yzH4gFEBFPIBg4Y5hKY8x0Y0ySMSYpPDzcwshKtRzdIlvRM7oVnyZrB7aqmmVFwhjzgDEmxhiTAFwDLDDGTK6w25fAFNfyla599ORtpRrIlf1j2HTgGFsP5tgdRTVSDX6dhIhME5GJrrtvAm1EZAdwD3B/Q+dRqiW7qHcUDg/RQ06qSp4N8SLGmIXAQtfyI+XWFwBXNUQGpdSZwoN8GNkpjJmr9/On8zvj5dDra9XpavSJEJGOIuLjWh4lIneJSIi10ZRSDeG6IfEcOlbIvI0H7Y6iGqGa/tkwEygVkUSch4jaAx9alkop1WBGd4mgXStfvlybZncU1QjVtEiUua5juAx4wRjzJyDSulhKqYbi4SGM79mOhdsytANbnaGmRaJYRCbhPBPpxBhMXtZEUko1tGsHx+Hn5eC3byzT6U3VaWpaJG4EhgJPGGN2iUh7oOI1D0qpJqpz2yCeuaI3mblFrNh1xqVKqgWrUZEwxmwyxtxljPlIREKBIGPM0xZnU0o1oHM6h+Hj6cE3Gw7YHUU1IjU9u2mhiLQSkdbAOuBtEfmntdGUUg3J39uTi3pF8sXqVI4VFNsdRzUSNT3cFGyMOQZcDrxtjBkAnGddLKWUHW4a0Z7jRaV8sVovrlNONS0SniISCVzNqY5rpVQz0zM6mG6Rrfh8jRYJ5VTTIjENmAekGGNWikgHYLt1sZRSdrm0bxTr9mWRmpVvdxTVCNS04/pTY0xvY8ztrvs7jTFXWBtNKWWHczo7R1pemqJzYKuad1zHiMgXIpIuIodEZKZr1jmlVDPTpW0Qof5eLEnJtDuKagRqerjpbZzDekfhnE3uK9c6pVQz4+EhDEsM46dtmZSW6cj9LV1Ni0S4MeZtY0yJ6/YOoLP/KNVMjevelszcQtbsPWp3FGWzmhaJTBGZLCIO120yoAcslWqmxnSNwNvhwbcbdGTYlq6mReImnKe/HgQO4JxF7karQiml7BXk68WITmHM3XAQnSyyZavp2U17jTETjTHhxpgIY8ylOC+sU0o1U+N7tiM1K5+NacfsjqJsVJ9pqO5xWwqlVKMzpmsEAIu2ZdicRNmpPkVC3JZCKdXohAX60COqlRaJFq4+RUIPVCrVzI3sFM7qPUfJLSyxO4qySbVFQkRyRORYJbccnNdMKKWasXM6h1FSZvTq6xas2iJhjAkyxrSq5BZkjPFsqJBKKXsMiA/F39vBwq3pdkdRNqnP4aZqiYiviKwQkXUislFEHqtknxtEJENE1rput1iVRylVez6eDs7v3pYv1qRyOLfQ7jjKBpYVCaAQGGOM6QP0BcaLyJBK9pthjOnrur1hYR6lVB38YUwihSVlXP/WCvKKtG+ipbGsSBinXNddL9dNO7uVamISI4L4z6R+bEw7xqw1aXbHUQ3MypYEriE81gLpwPfGmOWV7HaFiKwXkc9EJNbKPEqpupnQsx3dIlvx/rI9egV2C2NpkTDGlBpj+gIxwCAR6Vlhl6+ABGNMb2A+8G5lzyMiU0UkWUSSMzL0nG2lGpqI8NvBcWw6cIy1+7LsjqMakKVF4gRjTBawEBhfYf1hY8yJ3rDXgQFVPH66MSbJGJMUHq6Dzyplh0v7RRPg7eCD5XvtjqIakJVnN4WLSIhr2Q84D9hSYZ/IcncnAputyqOUqp9AH08u7RfNV+vSyMorsjuOaiBWtiQigR9FZD2wEmefxBwRmSYiE1373OU6PXYdcBdwg4V5lFL1NHlIPIUlZbz58y67o6gGYtkFccaY9UC/StY/Um75AeABqzIopdyrW2Qr/l+fKF5dlELrAG9uHN7e7kjKYg3SJ6GUaj6mTezB4PZtePrbLXrYqQXQIqGUqpXQAG8euqgbhSVlzFi5z+44ymJaJJRStdYtshWD2rfmvWV7KC3T6yaaMy0SSqk6mTI0gf1H8/lpu1671JxpkVBK1cl53SMI9vPii9WpdkdRFtIioZSqEx9PBxP7RDF340EOZhfYHUdZRIuEUqrObh3ZgbIyw6uLUuyOoiyiRUIpVWdxbfy5sFcks9amUlRSZnccZQEtEkqpepnYJ4qsvGIWbdMO7OZIi4RSql7O6RxOVLAvryzcocOIN0NaJJRS9eLt6cHtoxNZvTeLlbuP2h1HuZkWCaVUvV3ZP4YgX0/+t3S33VGUm2mRUErVm5+3g2sHx/H1rwfYkJptdxzlRloklFJucceoRFr7ezPlrRXsO5JndxzlJloklFJuEeznxYe3DuHw8SJmr9WrsJsLLRJKKbfp0i6I3jHBLNiSbncU5SZaJJRSbjW2a1vW7MvSQ07NhBYJpZRb/WZgLA4RneK0mdAioZRyq3bBvkzsE8UnyfvIziu2O46qJy0SSim3u2VkB/KKSvl0lc5c19RpkVBKuV33qFZ0j2zF3A0H7Y6i6kmLhFLKEuN6tGXV3qPszMi1O4qqBy0SSilLXNE/hhA/Lya/sVzPdGrCLCsSIuIrIitEZJ2IbBSRxyrZx0dEZojIDhFZLiIJVuVRSjWs2Nb+vHfzYI4XlXLnh6spLdMRYpsiK1sShcAYY0wfoC8wXkSGVNjnZuCoMSYR+BfwjIV5lFINrGd0MNMu6cH6/dl8uHyP3XFUHVhWJIzTiYORXq5bxT8lLgHedS1/BowVEbEqk1Kq4U3sE8Wwjm14dt5WMnIK7Y6jasnSPgkRcYjIWiAd+N4Ys7zCLtHAPgBjTAmQDbSp5HmmikiyiCRnZOjsV0o1JSLCtEt6UlBcyqTXl+kosU2MpUXCGFNqjOkLxACDRKRnhV0qazWcceDSGDPdGJNkjEkKDw+3IqpSykKJEYFMu6Qnew4f5+6P11CDVCpiAAARtUlEQVRSqvNhNxUNcnaTMSYLWAiMr7BpPxALICKeQDBwpCEyKaUa1qRBcbx0bX9SMo7z8Uq9yK6psPLspnARCXEt+wHnAVsq7PYlMMW1fCWwwOgkuUo1W+O6t2VQQmv+8d1WNqbpYaemwMqWRCTwo4isB1bi7JOYIyLTRGSia583gTYisgO4B7jfwjxKKZuJCE9f0QtfTwf3frre7jiqBjytemJjzHqgXyXrHym3XABcZVUGpVTj0yE8kDtGd+SR2RvZkJpNz+hguyOpaugV10qpBjexTxR+Xg6m/7TT7ijqLLRIKKUaXIi/NzcOT+DLdWlsO5RjdxxVDS0SSilb3DqyA75eHryxWFsTjZkWCaWULUIDvLk6KZZZa9JIP1ZgdxxVBS0SSinb3DyiPcVlZTrVaSOmRUIpZZv4NgFc3i+Gt37ZRYrOO9EoaZFQStnq/gld8fNy8MjsDei1tI2PFgmllK3Cg3z4y/iu/LLjMF+uS7M7jqpAi4RSynbXDoqjT0wwj8zeyI50PexUmSe+3sSCLYca/HW1SCilbOfwEP49qR8OD2HiSz/zSbIOAFje4dxCXl+8i80HGv6aEi0SSqlGIb5NAHP+MILeMcHcP3M9+4/qvNgnLNvpHBx7WMczptuxnBYJpVSjERXixz+u6gPAB8v32pym8ViSkkmgjye9bBjnSouEUqpRiQn1Z3zPdry3dA9HjhfZHadR2HM4j05tA/F0NPxXthYJpVSj86fzOpNfXMrt768iNSvf7ji2y8ovItTf25bX1iKhlGp0OrUN4p9X92HNviwu/vdidmUetzuSrbLyignx87LltbVIKKUapUv6RjP37pEY4MHPf7U7jq2y84oJ9tcioZRSp+kQHshdYzqxdOdhftjc8NcINAYlpWXkFJYQ4qeHm5RS6gzXDo6ja7sg/jhjLXM3HLA7ToM7VlACQLCfZROJVkuLhFKqUfP1cvD69Um0DwvgjzPWsvdwy7p+IivPeYZXiHZcK6VU5WJb+/PadQPw9PDgoVm/UlbWcgYCzMovBtA+CaWUqk5ksB9/Hd+Fxdsz6fK3b3nsq412R2oQ2XnOIqFnNyml1FlMHhLPC7/pS7CfF/9buoedLWAOiqz8Znq4SURiReRHEdksIhtF5O5K9hklItkistZ1e8SqPEqppk9EuLRfNN/cPRI/LwcPz2r+c1AcznUWidBmeLipBPizMaYbMAS4U0S6V7LfYmNMX9dtmoV5lFLNRESQLw9e2I0lKYf5cEXzHuNp75E8gnw9CW5uh5uMMQeMMatdyznAZiDaqtdTSrUskwbFMiIxjCe/3tysR4zdfTiPhDYBiIgtr98gfRIikgD0A5ZXsnmoiKwTkW9FpEcVj58qIskikpyRkWFhUqVUUyEiPHV5LwzwwOe/UtpMz3jac/g48W38bXt9y4uEiAQCM4E/GmOOVdi8Gog3xvQB/gPMquw5jDHTjTFJxpik8PBwawMrpZqM2Nb+PHBhNxZvz2Twk/O5f+Z60o8V2B3LbYpLy0g9mk9CmwDbMlh6CZ+IeOEsEB8YYz6vuL180TDGfCMiL4tImDEm08pcSqnmY/LgOMIDvZmz/gCfr0nlaF4Rr12XZHcst0jLyqekzDTPloQ4D6C9CWw2xvyzin3aufZDRAa58hy2KpNSqvkREcb3jOSla/tz56hE5m08xPxNzWOcp92uq8vjbWxJWHm4aThwHTCm3CmuF4rIbSJym2ufK4ENIrIO+DdwjWnu57MppSwz9ZwO9I4J5vYPVjF7bSrZrquVm6o9h51DpCfY2JKw7HCTMeZnoNrueGPMS8BLVmVQSrUsft4OXrymH2OfX8jdH6/F4SEkxYfy1OW96BAeaHe8WtudmYefl4PwIB/bMugV10qpZqV9WAAfTx3KC7/py+/O6cDWQznc8cFqjjbBqVD3HnGe2WTX6a9gcce1UkrZYVD71ieXBya05qZ3VzL6+YV8cMtgekQF25js7D5ZuY+UzFzKygzzN6czvkc7W/NoS0Ip1ayN7hrBN3c5h/H448drG/UIsjkFxdw3cz2vLdrJ64t3MTAhlLvP62RrJi0SSqlmr1tkKx68sBvb03P5fE2q3XEqZYzhn99vA2DK0HgW/WUUn942jG6RrWzNpUVCKdUiXNQrkgHxodz76Tquf2sFmw9UvLbXXt9vOsTbv+zmmoGxPDqxh62nvZanRUIp1SJ4eAgvXduPSYPi2JSWzVWvLmXl7iOWvV56TgFfrUtj26EcMnMLz1qU3l++l3atfPn7pT1t7aiuSDuulVItRmSwH09d3os/jElk8hvLuf7NFcz43RB6x4TU6nlyC0uYviiFX1OzuTopliUph+kR1YpVe44yeUg8z3+/jZ+2OceZ83Z44PAQ8otLue3cjtw/oetpz3W8sISPVuzlp20Z/OWCLng6Gtff7tLUrl1LSkoyycnJdsdQSjVx6ccKuOzlJXg5hPn3nFvjL+e3ft7F03O3UFRSVu1+NwxLYHzPdry7ZDcHsgsoM4YNqdm8f8tghnZog4gwe20qD3z+K3lFpYxIDOPtGwfiZVGREJFVxphaj1eiRUIp1WLN23iQ3723iol9orh/QlcignyqLRZlZYZBT/5AgI/zor3Uo/nkFBQzoVckaVn5+Hs7OPe5hQBs/ft4fDwdJx97rKCYy19ewo70XLwdHoQH+ZCalc/AhFDuOb8LQzq0tvQwU12LhB5uUkq1WOd3a8ukQXF8tmofX65LY0RiGO/cOLDKQrFm31Eycwv528V96RsbQt/YU4epTkwKNPP2oRSWlJ1WIABa+XoxY+oQ3lu2hyPHi0g9ms+1g+OYek4Hy1oP7qAtCaVUi7chNZuXFuxg7saDRIf48f4tg2kfdvrZRYUlpVzy0i+kZeWz+K9jbJsprq7q2pJovOVLKaUaSM/oYF6Z3J9/T+pHXlEJN769giOuYTyKS8tYsOUQbyzexZaDOTx3VZ8mVyDqQw83KaUUziHHJ/aJIjrEl0mvL+eWd1cyoWck/5q/jbyiUgBiW/txfre2NidtWFoklFKqnAHxrXnxN325/YPVrN6bBcDITmEMbt+a/vGheHg0nmsYGoIWCaWUqmBCr0jev3kwv6Rkcu2gOKJC/HC0sOJwghYJpZSqxIhOYYzoFGZ3DNtpx7VSSqkqaZFQSilVJS0SSimlqqRFQimlVJW0SCillKqSFgmllFJV0iKhlFKqSloklFJKVanJjQIrIhlAFpDtWhXsWj7xbxiQWYenPvH42mw/27qzLduRubL11d2vmLX8urrkbsjM5Zf181Hz7fr5qL2m8PmIN8aE12C/0xljmtwNmF5xudy/yfV9zppuP9u6sy3bkbmy9dXdr5i1vrkbMrPd77V+PvTz0Zg/HzW9NdXDTV9VsvxVZTvW8Tlruv1s6862bEfmytZXd7+yrPXJ3ZCZyy/r56Pm2/XzUXtN8fNRI03ucNPZiEiyqcPEGnZqipmhaebWzA2nKebWzGdqqi2J6ky3O0AdNMXM0DRza+aG0xRza+YKml1LQimllPs0x5aEUkopN2nURUJE3hKRdBHZUIfHDhCRX0Vkh4j8W0TEtX6GiKx13XaLyNrGntm17Q8islVENorIs+7M7Hp+K97rR0Uktdz7fWFjz1xu+70iYkTErRMKWPQ+Py4i613v8XciEtUEMj8nIltcub8QkRB3ZrYw91Wu38EyEXFbP0B9slbxfFNEZLvrNqXc+mo/95Wy8tSp+t6Ac4D+wIY6PHYFMBQQ4FtgQiX7PA880tgzA6OB+YCP635EU3ivgUeBe5va5wOIBeYBe4Cwxp4ZaFVun7uAV5tA5nGAp2v5GeCZpvD5ALoBXYCFQJLdWV05Eiqsaw3sdP0b6loOre7nqu7WqFsSxpifgCPl14lIRxGZKyKrRGSxiHSt+DgRicT5i7PUON+Z/wGXVthHgKuBj5pA5tuBp40xha7XSHdnZgtzW8rCzP8C7gPc3mFnRWZjzLFyuwa4O7dFmb8zxpS4dl0GxLgzs4W5NxtjtjaWrFW4APjeGHPEGHMU+B4YX9ff1UZdJKowHfiDMWYAcC/wciX7RAP7y93f71pX3kjgkDFmuyUpT1ffzJ2BkSKyXEQWichAS9Oe4o73+veuQwpviUiodVFPqldmEZkIpBpj1lkdtJx6v88i8oSI7AN+CzxiYdYT3PV7CHATzr9qG4I7c1utJlkrEw3sK3f/RP46/VxNao5rEQkEhgGfljuU5lPZrpWsq/jX1STc3IqojJsye+JsNg4BBgKfiEgH118DlnBT7leAx133H8d5eO8m9yYtF6SemUXEH3gI56GQBuGuz7Qx5iHgIRF5APg98H9ujnoqiBt/D0XkIaAE+MCdGSvj5u8PS1WXVURuBO52rUsEvhGRImCXMeYyqs5fp5+rSRUJnC2fLGNM3/IrRcQBrHLd/RLnl1P55msMkFZuf0/gcmCApWmd3JF5P/C5qyisEJEynOO1ZDTm3MaYQ+Ue9zowx8K8UP/MHYH2wDrXL2YMsFpEBhljDjbSzBV9CHyNhUUC9/0eTgEuBsZa+QdPOe5+r61UaVYAY8zbwNsAIrIQuMEYs7vcLvuBUeXux+Dsu9hPXX4ud3W8WHUDEijXmQMsAa5yLQvQp4rHrcT5l/eJDpoLy20bDyxqKpmB24BpruXOOJuS0gRyR5bb50/Ax409c4V9duPmjmuL3udO5fb5A/BZE8g8HtgEhLs7a0N8PnBzx3Vds1J1x/UunEcfQl3LrWv6uT/jNaz8D3LDm/YRcAAoxlkFb8b5l95cYJ3rQ1bp2UlAErABSAFeotyXKvAOcFtTyQx4A++7tq0GxjSR3O8BvwLrcf6FFtnYM1fYZzfuP7vJivd5pmv9epzj+UQ3gcw7cP6xs9Z1c+sZWRbmvsz1XIXAIWCenVmppEi41t/keo93ADfW5nNf8aZXXCullKpSUzy7SSmlVAPRIqGUUqpKWiSUUkpVSYuEUkqpKmmRUEopVSUtEqpZEJHcBn69N0Sku5ueq1ScI7huEJGv5CwjoopIiIjc4Y7XVups9BRY1SyISK4xJtCNz+dpTg1AZ6ny2UXkXWCbMeaJavZPAOYYY3o2RD7VsmlLQjVbIhIuIjNFZKXrNty1fpCILBGRNa5/u7jW3yAin4rIV8B3IjJKRBaKyGfinPvgA9fowbjWJ7mWc10D7K0TkWUi0ta1vqPr/koRmVbD1s5STg02GCgiP4jIanHOAXCJa5+ngY6u1sdzrn3/4nqd9SLymBvfRtXCaZFQzdmLwL+MMQOBK4A3XOu3AOcYY/rhHDH1yXKPGQpMMcaMcd3vB/wR6A50AIZX8joBwDJjTB/gJ+DWcq//ouv1zzpGjmsMobE4r04HKAAuM8b0xzmnyPOuInU/kGKM6WuM+YuIjAM6AYOAvsAAETnnbK+nVE00tQH+lKqN84Du5UbRbCUiQUAw8K6IdMI5CqZXucd8b4wpP67/CmPMfgBxzmKYAPxc4XWKODV44SrgfNfyUE6N1/8h8I8qcvqVe+5VOMf/B+f4Ok+6vvDLcLYw2lby+HGu2xrX/UCcReOnKl5PqRrTIqGaMw9gqDEmv/xKEfkP8KMx5jLX8f2F5TYfr/AcheWWS6n8d6bYnOrcq2qf6uQbY/qKSDDOYnMn8G+cc0OEAwOMMcUishvwreTxAjxljHmtlq+r1Fnp4SbVnH2Hc24FAETkxLDLwUCqa/kGC19/Gc7DXADXnG1nY0w2zulH7xURL5w5010FYjQQ79o1Bwgq99B5wE2uOQgQkWgRiXDTz6BaOC0SqrnwF5H95W734PzCTXJ15m7COeQ6wLPAUyLyC+CwMNMfgXtEZAUQCWSf7QHGmDU4R/28BudEPEkikoyzVbHFtc9h4BfXKbPPGWO+w3k4a6mI/Ap8xulFRKk601NglbKIa6a7fGOMEZFrgEnGmEvO9jilGhPtk1DKOgOAl1xnJGVh4dStSllFWxJKKaWqpH0SSimlqqRFQimlVJW0SCillKqSFgmllFJV0iKhlFKqSloklFJKVen/A80x8pTmgWVjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# There's a built-in tool in FastAI that helps identify the ideal learning rate. Generally, I use the LR one order of magnitude less tha the LR at the min loss.\n",
    "# (looking like about 2e-2).\n",
    "learn.lr_find(num_it=500);\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 13:42 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>rmse</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>1.682264</th>\n",
       "    <th>2.264273</th>\n",
       "    <th>1.912781</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>1.636734</th>\n",
       "    <th>4.306417</th>\n",
       "    <th>2.020808</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>1.577374</th>\n",
       "    <th>2.655945</th>\n",
       "    <th>1.730317</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>1.650120</th>\n",
       "    <th>2.506009</th>\n",
       "    <th>1.692205</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>1.665210</th>\n",
       "    <th>2.868882</th>\n",
       "    <th>1.855835</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the learer over 5 epochs using a one-cycle policy (origianl paper: https://arxiv.org/abs/1512.03385, great explination at:\n",
    "# https://sgugger.github.io/the-1cycle-policy.html) with a maximum learning rate of .02 and a weight decay value of 0.1.\n",
    "learn.fit(5, 2e-2, wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9472355"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate final RSME.\n",
    "x,y = learn.get_preds()\n",
    "rmse_output=np.sqrt(mean_squared_error(y, torch.sigmoid(x)));\n",
    "rmse_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "learn.save('for-slug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/fastai/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>259324</th>\n",
       "      <td>25.0</td>\n",
       "      <td>39.457664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259325</th>\n",
       "      <td>25.0</td>\n",
       "      <td>34.288387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259326</th>\n",
       "      <td>25.0</td>\n",
       "      <td>65.573067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259327</th>\n",
       "      <td>25.0</td>\n",
       "      <td>25.540644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259328</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.545351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259329</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.218024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259330</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.956040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259331</th>\n",
       "      <td>30.0</td>\n",
       "      <td>5.102535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259332</th>\n",
       "      <td>400.0</td>\n",
       "      <td>13.152665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259333</th>\n",
       "      <td>30.0</td>\n",
       "      <td>9.676002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259334</th>\n",
       "      <td>7.0</td>\n",
       "      <td>6.953747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259335</th>\n",
       "      <td>15.0</td>\n",
       "      <td>3.355512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259336</th>\n",
       "      <td>10.0</td>\n",
       "      <td>3.030128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259337</th>\n",
       "      <td>50.0</td>\n",
       "      <td>2.392354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259338</th>\n",
       "      <td>50.0</td>\n",
       "      <td>2.166795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            y      preds\n",
       "259324   25.0  39.457664\n",
       "259325   25.0  34.288387\n",
       "259326   25.0  65.573067\n",
       "259327   25.0  25.540644\n",
       "259328   10.0   2.545351\n",
       "259329    1.0   2.218024\n",
       "259330    1.0   1.956040\n",
       "259331   30.0   5.102535\n",
       "259332  400.0  13.152665\n",
       "259333   30.0   9.676002\n",
       "259334    7.0   6.953747\n",
       "259335   15.0   3.355512\n",
       "259336   10.0   3.030128\n",
       "259337   50.0   2.392354\n",
       "259338   50.0   2.166795"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a copy of the validation set\n",
    "temp = learn.get_preds()\n",
    "preds = np.exp(temp[0].data).numpy().T[0]\n",
    "valid_temp = df.iloc[valid_idx]\n",
    "\n",
    "# Add the predictions to the data set\n",
    "valid_temp['preds'] = preds\n",
    "\n",
    "# Show slices of the data, focusing just on the pred vs. actuals. \n",
    "valid_temp.iloc[15:30,-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there's some work still to do. My supposition is that the skew in values is still too hard for the model to reflect. I'm going to try a few things:\n",
    "\n",
    "  * Dropping values more than 3 sigma from the mean (I think this will the model better in lower-value cases, but crap at higher values).\n",
    "  * Creating three models - a classifier that groups values into above or below XX value (10c?), then two regression models for each value range.\n",
    "  \n",
    "I hope to send you the next version in a week or two. Please send any suggestions my way!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
